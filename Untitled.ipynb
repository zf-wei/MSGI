{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3fddefa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n",
      "|_1 |_2  |_3  |_4  |_5  |_6  |_7  |_8  |_9  |_10 |_11 |_12 |_13 |_14 |_15 |_16 |_17 |_18 |_19 |_20 |\n",
      "+---+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n",
      "|1.2|3.4 |5.6 |7.8 |9.0 |11.2|13.4|15.6|17.8|20.0|22.2|24.4|26.6|28.8|31.0|33.2|35.4|37.6|39.8|42.0|\n",
      "|2.3|4.5 |6.7 |8.9 |11.1|13.3|15.5|17.7|19.9|22.1|24.3|26.5|28.7|30.9|33.1|35.3|37.5|39.7|41.9|44.1|\n",
      "|3.4|5.6 |7.8 |10.0|12.2|14.4|16.6|18.8|21.0|23.2|25.4|27.6|29.8|32.0|34.2|36.4|38.6|40.8|43.0|45.2|\n",
      "|4.5|6.7 |8.9 |11.1|13.3|15.5|17.7|19.9|22.1|24.3|26.5|28.7|30.9|33.1|35.3|37.5|39.7|41.9|44.1|46.3|\n",
      "|5.6|7.8 |10.0|12.2|14.4|16.6|18.8|21.0|23.2|25.4|27.6|29.8|32.0|34.2|36.4|38.6|40.8|43.0|45.2|47.4|\n",
      "|6.7|8.9 |11.1|13.3|15.5|17.7|19.9|22.1|24.3|26.5|28.7|30.9|33.1|35.3|37.5|39.7|41.9|44.1|46.3|48.5|\n",
      "|7.8|10.0|12.2|14.4|16.6|18.8|21.0|23.2|25.4|27.6|29.8|32.0|34.2|36.4|38.6|40.8|43.0|45.2|47.4|49.6|\n",
      "+---+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    [1.2, 3.4, 5.6, 7.8, 9.0, 11.2, 13.4, 15.6, 17.8, 20.0,\n",
    "     22.2, 24.4, 26.6, 28.8, 31.0, 33.2, 35.4, 37.6, 39.8, 42.0],\n",
    "    [2.3, 4.5, 6.7, 8.9, 11.1, 13.3, 15.5, 17.7, 19.9, 22.1,\n",
    "     24.3, 26.5, 28.7, 30.9, 33.1, 35.3, 37.5, 39.7, 41.9, 44.1],\n",
    "    [3.4, 5.6, 7.8, 10.0, 12.2, 14.4, 16.6, 18.8, 21.0, 23.2,\n",
    "     25.4, 27.6, 29.8, 32.0, 34.2, 36.4, 38.6, 40.8, 43.0, 45.2],\n",
    "    [4.5, 6.7, 8.9, 11.1, 13.3, 15.5, 17.7, 19.9, 22.1, 24.3,\n",
    "     26.5, 28.7, 30.9, 33.1, 35.3, 37.5, 39.7, 41.9, 44.1, 46.3],\n",
    "    [5.6, 7.8, 10.0, 12.2, 14.4, 16.6, 18.8, 21.0, 23.2, 25.4,\n",
    "     27.6, 29.8, 32.0, 34.2, 36.4, 38.6, 40.8, 43.0, 45.2, 47.4],\n",
    "    [6.7, 8.9, 11.1, 13.3, 15.5, 17.7, 19.9, 22.1, 24.3, 26.5,\n",
    "     28.7, 30.9, 33.1, 35.3, 37.5, 39.7, 41.9, 44.1, 46.3, 48.5],\n",
    "    [7.8, 10.0, 12.2, 14.4, 16.6, 18.8, 21.0, 23.2, 25.4, 27.6,\n",
    "     29.8, 32.0, 34.2, 36.4, 38.6, 40.8, 43.0, 45.2, 47.4, 49.6]]\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Create a DataFrame with a column as a vector\n",
    "df = spark.createDataFrame(data)\n",
    "\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show(truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d33ce582",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/06/14 19:56:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    [1.2, 2.3, 3.4, 4.5, 5.6, 6.7, 7.8],\n",
    "    [3.4, 4.5, 5.6, 6.7, 7.8, 8.9, 10.0],\n",
    "    [5.6, 6.7, 7.8, 8.9, 10.0, 11.1, 12.2],\n",
    "    [7.8, 8.9, 10.0, 11.1, 12.2, 13.3, 14.4],\n",
    "    [9.0, 11.1, 12.2, 13.3, 14.4, 15.5, 16.6],\n",
    "    [11.2, 13.3, 14.4, 15.5, 16.6, 17.7, 18.8],\n",
    "    [13.4, 15.5, 16.6, 17.7, 18.8, 19.9, 21.0]\n",
    "]\n",
    "\n",
    "vectors = [Vectors.dense(row) for row in data]\n",
    "df = spark.createDataFrame([(vector,) for vector in vectors], [\"features\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07293403",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+\n",
      "|features                            |\n",
      "+------------------------------------+\n",
      "|[1.2,2.3,3.4,4.5,5.6,6.7,7.8]       |\n",
      "|[3.4,4.5,5.6,6.7,7.8,8.9,10.0]      |\n",
      "|[5.6,6.7,7.8,8.9,10.0,11.1,12.2]    |\n",
      "|[7.8,8.9,10.0,11.1,12.2,13.3,14.4]  |\n",
      "|[9.0,11.1,12.2,13.3,14.4,15.5,16.6] |\n",
      "|[11.2,13.3,14.4,15.5,16.6,17.7,18.8]|\n",
      "|[13.4,15.5,16.6,17.7,18.8,19.9,21.0]|\n",
      "+------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
