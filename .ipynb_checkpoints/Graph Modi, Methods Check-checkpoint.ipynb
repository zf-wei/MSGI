{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4ab65f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bf48d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 使用 networkx 包中的函数 LFR_benchmark_graph 生成随机图\n",
    "import networkx as nx\n",
    "from networkx.generators.community import LFR_benchmark_graph\n",
    "\n",
    "n = 1000\n",
    "tau1 = 2  # Power-law exponent for the degree distribution\n",
    "tau2 = 1.1 # Power-law exponent for the community size distribution \n",
    "            #S hould be >1\n",
    "mu = 0.05 # Mixing parameter\n",
    "avg_deg = 25 # Average Degree\n",
    "max_deg = 100 # Max Degree\n",
    "min_commu = 80 # Min Community Size\n",
    "max_commu = 100 # Max Community Size\n",
    "\n",
    "G = LFR_benchmark_graph(\n",
    "    n, tau1, tau2, mu, average_degree=avg_deg, max_degree=max_deg, min_community=min_commu, max_community=max_commu, \n",
    "    seed=7\n",
    ")\n",
    "### 去掉 G 中的重边和自环 \n",
    "G = nx.Graph(G) # Remove multi-edges\n",
    "\n",
    "selfloop_edges = list(nx.selfloop_edges(G)) # a list of self loops\n",
    "\n",
    "G.remove_edges_from(selfloop_edges) # Remove self-loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b98f836",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "intrinsic_communities = {frozenset(G.nodes[v][\"community\"]) for v in G}\n",
    "intrinsic_membership = np.empty(G.number_of_nodes(), dtype=int)\n",
    "for node in range(G.number_of_nodes()):\n",
    "    for index, inner_set in enumerate(intrinsic_communities):\n",
    "        if node in inner_set:\n",
    "            intrinsic_membership[node] = index\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa7f720d",
   "metadata": {},
   "outputs": [],
   "source": [
    "G.remove_node(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5276cdab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NodeView((0, 1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975, 976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be5cbc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = [True] *(G.number_of_nodes()+1)\n",
    "idx[2]=False\n",
    "intrinsic_membership=intrinsic_membership[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67cc5a2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.unique(intrinsic_membership))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bab96c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NMI: 0.7899093216796161\n"
     ]
    }
   ],
   "source": [
    "D=12\n",
    "### 1 Hope 方法\n",
    "from auxpack.evaluate_embd702temp import evaluate_embd as EEE\n",
    "from gem.embedding.hope import HOPE    \n",
    "hope_model = HOPE(d=D, beta=0.01) \n",
    "# A higher value of beta places more emphasis on capturing higher-order proximities\n",
    "embd = hope_model.learn_embedding(graph=G, is_weighted=False, no_python=True)\n",
    "defen = EEE(11, intrinsic_membership, embd)\n",
    "print(\"NMI:\", defen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af333798",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NMI: 0.7757197101483935\n"
     ]
    }
   ],
   "source": [
    "D=89\n",
    "### 2 Laplacian 方法\n",
    "from gem.embedding.lap import LaplacianEigenmaps\n",
    "\n",
    "lap_model = LaplacianEigenmaps(d=D)\n",
    "embd = lap_model.learn_embedding(graph=G, is_weighted=False, no_python=True)\n",
    "defen = EEE(11, intrinsic_membership, embd)\n",
    "print(\"NMI:\", defen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "92cb0ebe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NMI: 1.0\n",
      "CPU times: user 16.7 s, sys: 13.2 s, total: 29.9 s\n",
      "Wall time: 1.43 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from karateclub import MNMF\n",
    "D=15\n",
    "\n",
    "K = max(intrinsic_membership)+1\n",
    "\n",
    "\n",
    "# Create an instance of the MNMF model\n",
    "MNMF_model = MNMF(dimensions = D, clusters = K, lambd = 0.2, \n",
    "             alpha = 0.05, beta = 0.05, iterations = 100, \n",
    "             lower_control = 1e-15, eta = 5.0, seed = 42)\n",
    "\n",
    "# Fit the model to the graph\n",
    "MNMF_model.fit(H)\n",
    "\n",
    "# Obtain the graph embeddings\n",
    "embd = MNMF_model.get_embedding()\n",
    "\n",
    "defen = EEE(11, intrinsic_membership, embd)\n",
    "print(\"NMI:\", defen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2971b979",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LLE 方法可以"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62e6bcb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NMI: 1.0\n"
     ]
    }
   ],
   "source": [
    "from node2vec import Node2Vec\n",
    "import numpy as np\n",
    "\n",
    "nodes = [str(i) for i in list(G.nodes())]\n",
    "\n",
    "# Precompute probabilities and generate walks - **ON WINDOWS ONLY WORKS WITH workers=1**\n",
    "node2vec_model = Node2Vec(G, dimensions=5, walk_length=16, num_walks=8, workers=32, quiet=True) #, temp_folder='test' # Use temp_folder for big graphs\n",
    "# Embed nodes \n",
    "node2vec_fit = node2vec_model.fit(window=10, min_count=1, batch_words=16192)  \n",
    "# Any keywords acceptable by gensim.Word2Vec can be passed, `dimensions` and `workers` are automatically passed \n",
    "# (from the Node2Vec constructor)\n",
    "embd = np.array([node2vec_fit.wv[node] for node in nodes])\n",
    "defen = EEE(11, intrinsic_membership, embd)\n",
    "print(\"NMI:\", defen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0072b513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NMI: 0.8146574950869895\n"
     ]
    }
   ],
   "source": [
    "#from karateclub import DeepWalk\n",
    "\n",
    "model = DeepWalk(dimensions=3, walk_length=16, window_size=10)\n",
    "model.fit(G)\n",
    "embd = model.get_embedding()\n",
    "defen = EEE(11, intrinsic_membership, embd)\n",
    "print(\"NMI:\", defen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "41252359",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NMI: 0.6693079275527246\n",
      "NMI: 0.942077257087706\n",
      "NMI: 0.9921851055453348\n",
      "NMI: 0.9976925106779044\n",
      "NMI: 0.9999999999999998\n",
      "NMI: 1.0\n",
      "NMI: 1.0000000000000002\n",
      "NMI: 0.9999999999999998\n",
      "NMI: 1.0\n",
      "NMI: 1.0\n",
      "NMI: 1.0\n",
      "NMI: 1.0\n",
      "NMI: 0.9999999999999998\n",
      "NMI: 1.0\n",
      "NMI: 0.9999999999999998\n",
      "NMI: 1.0\n",
      "NMI: 1.0\n",
      "NMI: 1.0\n",
      "NMI: 1.0\n",
      "NMI: 1.0\n",
      "NMI: 1.0\n",
      "NMI: 1.0\n",
      "NMI: 0.9999999999999998\n",
      "NMI: 1.0\n"
     ]
    }
   ],
   "source": [
    "from ge import LINE\n",
    "for D in range(2,50,2):\n",
    "    model = LINE(G,embedding_size=D,order='first');\n",
    "    model.train(batch_size=8192,epochs=50,verbose=0);# train model\n",
    "    LINE_embd = model.get_embeddings();# get embedding vectors\n",
    "\n",
    "    embd = list(LINE_embd.values())\n",
    "    defen = EEE(11, intrinsic_membership, embd)\n",
    "    print(\"NMI:\", defen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "06219281",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import warnings\n",
    "from typing import List\n",
    "import re\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from karateclub.utils.walker import RandomWalker\n",
    "\n",
    "\n",
    "class DeepWalk:\n",
    "    r\"\"\"An implementation of `\"DeepWalk\" <https://arxiv.org/abs/1403.6652>`_\n",
    "    from the KDD '14 paper \"DeepWalk: Online Learning of Social Representations\".\n",
    "    The procedure uses random walks to approximate the pointwise mutual information\n",
    "    matrix obtained by pooling normalized adjacency matrix powers. This matrix\n",
    "    is decomposed by an approximate factorization technique.\n",
    "\n",
    "    Args:\n",
    "        walk_number (int): Number of random walks. Default is 10.\n",
    "        walk_length (int): Length of random walks. Default is 80.\n",
    "        dimensions (int): Dimensionality of embedding. Default is 128.\n",
    "        workers (int): Number of cores. Default is 4.\n",
    "        window_size (int): Matrix power order. Default is 5.\n",
    "        epochs (int): Number of epochs. Default is 1.\n",
    "        use_hierarchical_softmax (bool): Whether to use hierarchical softmax or negative sampling to train the model. Default is True.\n",
    "        number_of_negative_samples (int): Number of negative nodes to sample (usually between 5-20). If set to 0, no negative sampling is used. Default is 5.\n",
    "        learning_rate (float): HogWild! learning rate. Default is 0.05.\n",
    "        min_count (int): Minimal count of node occurrences. Default is 1.\n",
    "        seed (int): Random seed value. Default is 42.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        walk_number: int = 10,\n",
    "        walk_length: int = 80,\n",
    "        dimensions: int = 128,\n",
    "        workers: int = 4,\n",
    "        window_size: int = 5,\n",
    "        epochs: int = 1,\n",
    "        use_hierarchical_softmax: bool = True,\n",
    "        number_of_negative_samples: int = 5,\n",
    "        learning_rate: float = 0.05,\n",
    "        min_count: int = 1,\n",
    "        seed: int = 42,\n",
    "    ):\n",
    "        self.walk_number = walk_number\n",
    "        self.walk_length = walk_length\n",
    "        self.dimensions = dimensions\n",
    "        self.workers = workers\n",
    "        self.window_size = window_size\n",
    "        self.epochs = epochs\n",
    "        self.use_hierarchical_softmax = use_hierarchical_softmax\n",
    "        self.number_of_negative_samples = number_of_negative_samples\n",
    "        self.learning_rate = learning_rate\n",
    "        self.min_count = min_count\n",
    "        self.seed = seed\n",
    "\n",
    "    def fit(self, graph: nx.classes.graph.Graph):\n",
    "        \"\"\"\n",
    "        Fitting a DeepWalk model.\n",
    "\n",
    "        Arg types:\n",
    "            * **graph** *(NetworkX graph)* - The graph to be embedded.\n",
    "        \"\"\"\n",
    "        self._set_seed()\n",
    "        graph = self._check_graph(graph)\n",
    "        walker = RandomWalker(self.walk_length, self.walk_number)\n",
    "        walker.do_walks(graph)\n",
    "\n",
    "        model = Word2Vec(\n",
    "            walker.walks,\n",
    "            hs=1 if self.use_hierarchical_softmax else 0,\n",
    "            negative=self.number_of_negative_samples,\n",
    "            alpha=self.learning_rate,\n",
    "            epochs=self.epochs,\n",
    "            vector_size=self.dimensions,\n",
    "            window=self.window_size,\n",
    "            min_count=self.min_count,\n",
    "            workers=self.workers,\n",
    "            seed=self.seed,\n",
    "        )\n",
    "\n",
    "        num_of_nodes = graph.number_of_nodes()\n",
    "        self._embedding = [model.wv[str(n)] for n in range(num_of_nodes)]\n",
    "\n",
    "    def get_embedding(self) -> np.array:\n",
    "        r\"\"\"Getting the node embedding.\n",
    "\n",
    "        Return types:\n",
    "            * **embedding** *(Numpy array)* - The embedding of nodes.\n",
    "        \"\"\"\n",
    "        return np.array(self._embedding)\n",
    "\n",
    "    def get_params(self):\n",
    "        \"\"\"Get parameter dictionary for this estimator..\"\"\"\n",
    "        rx = re.compile(r'^\\_')\n",
    "        params = self.__dict__\n",
    "        params = {key: params[key] for key in params if not rx.search(key)}\n",
    "        return params\n",
    "\n",
    "    def _set_seed(self):\n",
    "        \"\"\"Creating the initial random seed.\"\"\"\n",
    "        random.seed(self.seed)\n",
    "        np.random.seed(self.seed)\n",
    "\n",
    "    @staticmethod\n",
    "    def _ensure_walk_traversal_conditions(graph: nx.classes.graph.Graph) -> nx.classes.graph.Graph:\n",
    "        \"\"\"Ensure walk traversal conditions.\"\"\"\n",
    "        # We create a copy of the graph\n",
    "        graph = graph.copy()\n",
    "        for node_index in list(graph.nodes()):\n",
    "            if not graph.has_edge(node_index, node_index):\n",
    "                # And we add the missing edges\n",
    "                # for filling the main diagonal\n",
    "                graph.add_edges_from(\n",
    "                    [(index, index) for index in list(graph.nodes()) if not graph.has_edge(index, index)]\n",
    "                )\n",
    "                break\n",
    "\n",
    "        return graph\n",
    "\n",
    "    @staticmethod\n",
    "    def _check_indexing(graph: nx.classes.graph.Graph):\n",
    "        \"\"\"Checking the consecutive numeric indexing.\"\"\"\n",
    "        numeric_indices = [index for index in range(graph.number_of_nodes())]\n",
    "        node_indices = sorted([node for node in graph.nodes()])\n",
    "\n",
    "        assert numeric_indices == node_indices, \"The node indexing is wrong.\"\n",
    "\n",
    "    def _check_graph(self, graph: nx.classes.graph.Graph) -> nx.classes.graph.Graph:\n",
    "        \"\"\"Check the Karate Club assumptions about the graph.\"\"\"\n",
    "        #self._check_indexing(graph)\n",
    "        graph = self._ensure_walk_traversal_conditions(graph)\n",
    "\n",
    "        return graph\n",
    "\n",
    "    def _check_graphs(self, graphs: List[nx.classes.graph.Graph]):\n",
    "        \"\"\"Check the Karate Club assumptions for a list of graphs.\"\"\"\n",
    "        graphs = [self._check_graph(graph) for graph in graphs]\n",
    "\n",
    "        return graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "80259f0a",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 56)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m<tokenize>:56\u001b[0;36m\u001b[0m\n\u001b[0;31m    def _modularity_generator(self):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import warnings\n",
    "from typing import List\n",
    "from tqdm.auto import trange\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from typing import Dict\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "\n",
    "\n",
    "class MNMF:\n",
    "    r\"\"\"An implementation of `\"M-NMF\" <https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14589/13763>`_\n",
    "    from the AAAI '17 paper \"Community Preserving Network Embedding\".\n",
    "    The procedure uses joint non-negative matrix factorization with modularity\n",
    "    based regularization in order to learn a cluster membership distribution\n",
    "    over nodes. The method can be used in an overlapping and non-overlapping way.\n",
    "\n",
    "    Args:\n",
    "        dimensions (int): Number of dimensions. Default is 128.\n",
    "        clusters (int): Number of clusters. Default is 10.\n",
    "        lambd (float): KKT penalty. Default is 0.2\n",
    "        alpha (float): Clustering penalty. Default is 0.05.\n",
    "        beta (float): Modularity regularization penalty. Default is 0.05.\n",
    "        iterations (int): Number of power iterations. Default is 200.\n",
    "        lower_control (float): Floating point overflow control. Default is 10**-15.\n",
    "        eta (float): Similarity mixing parameter. Default is 5.0.\n",
    "        seed (int): Random seed value. Default is 42.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dimensions: int = 128,\n",
    "        clusters: int = 10,\n",
    "        lambd: float = 0.2,\n",
    "        alpha: float = 0.05,\n",
    "        beta: float = 0.05,\n",
    "        iterations: int = 200,\n",
    "        lower_control: float = 10**-15,\n",
    "        eta: float = 5.0,\n",
    "        seed: int = 42,\n",
    "    ):\n",
    "\n",
    "        self.dimensions = dimensions\n",
    "        self.clusters = clusters\n",
    "        self.lambd = lambd\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.iterations = iterations\n",
    "        self.lower_control = lower_control\n",
    "        self.eta = eta\n",
    "        self.seed = seed\n",
    "\n",
    "   def _modularity_generator(self):\n",
    "    \"\"\"Calculating the sparse modularity matrix.\"\"\"\n",
    "    node_indices = list(self._graph.nodes())\n",
    "    node_mapping = {node: index for index, node in enumerate(node_indices)}\n",
    "    inv_node_mapping = {index: node for node, index in node_mapping.items()}\n",
    "    e_count = self._graph.number_of_edges()\n",
    "    n_count = self._graph.number_of_nodes()\n",
    "    modularity_mat_shape = (n_count, n_count)\n",
    "    indices_1 = np.array(\n",
    "        [node_mapping[edge[0]] for edge in self._graph.edges()]\n",
    "        + [node_mapping[edge[1]] for edge in self._graph.edges()]\n",
    "    )\n",
    "    indices_2 = np.array(\n",
    "        [node_mapping[edge[1]] for edge in self._graph.edges()]\n",
    "        + [node_mapping[edge[0]] for edge in self._graph.edges()]\n",
    "    )\n",
    "    scores = [\n",
    "        float(self._graph.degree(inv_node_mapping[e[0]]) * self._graph.degree(inv_node_mapping[e[1]])) / (2 * e_count)\n",
    "        for e in self._graph.edges()\n",
    "    ]\n",
    "    scores = scores + [\n",
    "        float(self._graph.degree(inv_node_mapping[e[1]]) * self._graph.degree(inv_node_mapping[e[0]])) / (2 * e_count)\n",
    "        for e in self._graph.edges()\n",
    "    ]\n",
    "    mod_matrix = coo_matrix(\n",
    "        (scores, (indices_1, indices_2)), shape=modularity_mat_shape\n",
    "    )\n",
    "    return mod_matrix\n",
    "\n",
    "\n",
    "    def _setup_matrices(self):\n",
    "        \"\"\"Creating parameter matrices and target matrices.\"\"\"\n",
    "        self._number_of_nodes = nx.number_of_nodes(self._graph)\n",
    "        self._M = np.random.uniform(0, 1, (self._number_of_nodes, self.dimensions))\n",
    "        self._U = np.random.uniform(0, 1, (self._number_of_nodes, self.dimensions))\n",
    "        self._H = np.random.uniform(0, 1, (self._number_of_nodes, self.clusters))\n",
    "        self._C = np.random.uniform(0, 1, (self.clusters, self.dimensions))\n",
    "        self._B1 = nx.adjacency_matrix(\n",
    "            self._graph, nodelist=list(self._graph.nodes())\n",
    "        )\n",
    "        self._B2 = self._modularity_generator()\n",
    "        self._X = np.transpose(self._U)\n",
    "        overlaps = self._B1.dot(self._B1)\n",
    "        self._S = self._B1 + self.eta * self._B1 * (overlaps)\n",
    "\n",
    "    def _update_M(self):\n",
    "        \"\"\"Update matrix M.\"\"\"\n",
    "        enum = self._S.dot(self._U)\n",
    "        denom = np.dot(self._M, np.dot(np.transpose(self._U), self._U))\n",
    "        denom[denom < self.lower_control] = self.lower_control\n",
    "        self._M = np.multiply(self._M, enum / denom)\n",
    "        row_sums = self._M.sum(axis=1)\n",
    "        self._M = self._M / row_sums[:, np.newaxis]\n",
    "\n",
    "    def _update_U(self):\n",
    "        \"\"\"Update matrix U.\"\"\"\n",
    "        enum = self._S.dot(self._M) + self.alpha * np.dot(self._H, self._C)\n",
    "        denom = np.dot(\n",
    "            self._U,\n",
    "            np.dot(np.transpose(self._M), self._M)\n",
    "            + self.alpha * np.dot(np.transpose(self._C), self._C),\n",
    "        )\n",
    "        denom[denom < self.lower_control] = self.lower_control\n",
    "        self._U = np.multiply(self._U, enum / denom)\n",
    "        row_sums = self._U.sum(axis=1)\n",
    "        self._U = self._U / row_sums[:, np.newaxis]\n",
    "\n",
    "    def _update_C(self):\n",
    "        \"\"\"Update matrix C.\"\"\"\n",
    "        enum = np.dot(np.transpose(self._H), self._U)\n",
    "        denom = np.dot(self._C, np.dot(np.transpose(self._U), self._U))\n",
    "        denom[denom < self.lower_control] = self.lower_control\n",
    "        frac = enum / denom\n",
    "        self._C = np.multiply(self._C, frac)\n",
    "        row_sums = self._C.sum(axis=1)\n",
    "        self._C = self._C / row_sums[:, np.newaxis]\n",
    "\n",
    "    def _update_H(self):\n",
    "        \"\"\"Update matrix H.\"\"\"\n",
    "        B1H = self._B1.dot(self._H)\n",
    "        B2H = self._B2.dot(self._H)\n",
    "        HHH = np.dot(self._H, (np.dot(np.transpose(self._H), self._H)))\n",
    "        UC = np.dot(self._U, np.transpose(self._C))\n",
    "        rooted = np.square(2 * self.beta * B2H) + np.multiply(\n",
    "            16 * self.lambd * HHH,\n",
    "            (\n",
    "                2 * self.beta * B1H\n",
    "                + 2 * self.alpha * UC\n",
    "                + (4 * self.lambd - 2 * self.alpha) * self._H\n",
    "            ),\n",
    "        )\n",
    "        rooted[rooted < 0] = 0\n",
    "        sqroot_1 = np.sqrt(rooted)\n",
    "        enum = -2 * self.beta * B2H + sqroot_1\n",
    "        denom = 8 * self.lambd * HHH\n",
    "        denom[denom < self.lower_control] = self.lower_control\n",
    "        rooted = enum / denom\n",
    "        rooted[rooted < 0] = 0\n",
    "        sqroot_2 = np.sqrt(rooted)\n",
    "        self._H = np.multiply(self._H, sqroot_2)\n",
    "        row_sums = self._H.sum(axis=1)\n",
    "        self._H = self._H / row_sums[:, np.newaxis]\n",
    "\n",
    "    def get_memberships(self) -> Dict[int, int]:\n",
    "        r\"\"\"Getting the cluster membership of nodes.\n",
    "\n",
    "        Return types:\n",
    "            * **memberships** *(dict)* - Node cluster memberships.\n",
    "        \"\"\"\n",
    "        indices = np.argmax(self._H, axis=1)\n",
    "        memberships = {i: membership for i, membership in enumerate(indices)}\n",
    "        return memberships\n",
    "\n",
    "    def get_embedding(self) -> np.array:\n",
    "        r\"\"\"Getting the node embedding.\n",
    "\n",
    "        Return types:\n",
    "            * **embedding** *(Numpy array)* - The embedding of nodes.\n",
    "        \"\"\"\n",
    "        embedding = self._U\n",
    "        return embedding\n",
    "\n",
    "    def get_cluster_centers(self) -> np.array:\n",
    "        r\"\"\"Getting the node embedding.\n",
    "\n",
    "        Return types:\n",
    "            * **centers** *(Numpy array)* - The cluster centers.\n",
    "        \"\"\"\n",
    "        centers = self._C\n",
    "        return centers\n",
    "\n",
    "    def fit(self, graph: nx.classes.graph.Graph):\n",
    "        \"\"\"\n",
    "        Fitting an M-NMF clustering model.\n",
    "\n",
    "        Arg types:\n",
    "            * **graph** *(NetworkX graph)* - The graph to be clustered.\n",
    "        \"\"\"\n",
    "        self._set_seed()\n",
    "        graph = self._check_graph(graph)\n",
    "        self._graph = graph\n",
    "        self._setup_matrices()\n",
    "        for _ in range(self.iterations):\n",
    "            self._update_M()\n",
    "            self._update_U()\n",
    "            self._update_C()\n",
    "            self._update_H()\n",
    "\n",
    "    def get_params(self):\n",
    "        \"\"\"Get parameter dictionary for this estimator..\"\"\"\n",
    "        rx = re.compile(r'^\\_')\n",
    "        params = self.__dict__\n",
    "        params = {key: params[key] for key in params if not rx.search(key)}\n",
    "        return params\n",
    "\n",
    "    def _set_seed(self):\n",
    "        \"\"\"Creating the initial random seed.\"\"\"\n",
    "        random.seed(self.seed)\n",
    "        np.random.seed(self.seed)\n",
    "\n",
    "    @staticmethod\n",
    "    def _ensure_walk_traversal_conditions(graph: nx.classes.graph.Graph) -> nx.classes.graph.Graph:\n",
    "        \"\"\"Ensure walk traversal conditions.\"\"\"\n",
    "        # We create a copy of the graph\n",
    "        graph = graph.copy()\n",
    "        for node_index in list(graph.nodes()):\n",
    "            if not graph.has_edge(node_index, node_index):\n",
    "                # And we add the missing edges\n",
    "                # for filling the main diagonal\n",
    "                graph.add_edges_from(\n",
    "                    [(index, index) for index in list(graph.nodes()) if not graph.has_edge(index, index)]\n",
    "                )\n",
    "                break\n",
    "\n",
    "        return graph\n",
    "\n",
    "    @staticmethod\n",
    "    def _check_indexing(graph: nx.classes.graph.Graph):\n",
    "        \"\"\"Checking the consecutive numeric indexing.\"\"\"\n",
    "        numeric_indices = [index for index in range(graph.number_of_nodes())]\n",
    "        node_indices = sorted([node for node in graph.nodes()])\n",
    "\n",
    "        assert numeric_indices == node_indices, \"The node indexing is wrong.\"\n",
    "\n",
    "    def _check_graph(self, graph: nx.classes.graph.Graph) -> nx.classes.graph.Graph:\n",
    "        \"\"\"Check the Karate Club assumptions about the graph.\"\"\"\n",
    "        #self._check_indexing(graph)\n",
    "        graph = self._ensure_walk_traversal_conditions(graph)\n",
    "\n",
    "        return graph\n",
    "\n",
    "    def _check_graphs(self, graphs: List[nx.classes.graph.Graph]):\n",
    "        \"\"\"Check the Karate Club assumptions for a list of graphs.\"\"\"\n",
    "        graphs = [self._check_graph(graph) for graph in graphs]\n",
    "\n",
    "        return graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "84e32f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "G0 = LFR_benchmark_graph(\n",
    "    n, tau1, tau2, mu, average_degree=avg_deg, max_degree=max_deg, min_community=min_commu, max_community=max_commu, \n",
    "    seed=7\n",
    ")\n",
    "### 去掉 G 中的重边和自环 \n",
    "G0 = nx.Graph(G0) # Remove multi-edges\n",
    "\n",
    "selfloop_edges = list(nx.selfloop_edges(G0)) # a list of self loops\n",
    "\n",
    "G0.remove_edges_from(selfloop_edges) # Remove self-loops\n",
    "\n",
    "import numpy as np\n",
    "intrinsic_communities0 = {frozenset(G0.nodes[v][\"community\"]) for v in G}\n",
    "intrinsic_membership0 = np.empty(G0.number_of_nodes(), dtype=int)\n",
    "for node in range(G0.number_of_nodes()):\n",
    "    for index, inner_set in enumerate(intrinsic_communities0):\n",
    "        if node in inner_set:\n",
    "            intrinsic_membership0[node] = index\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e03dcaa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NMI: 0.9881605067085244\n"
     ]
    }
   ],
   "source": [
    "model = DeepWalk(dimensions=4, walk_length=16, window_size=10)\n",
    "model.fit(G0)\n",
    "embd = model.get_embedding()\n",
    "defen = EEE(11, intrinsic_membership0, embd)\n",
    "print(\"NMI:\", defen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9d391fcd",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "row index exceeds matrix dimensions",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [81]\u001b[0m, in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m MNMF_model \u001b[38;5;241m=\u001b[39m MNMF(dimensions \u001b[38;5;241m=\u001b[39m D, clusters \u001b[38;5;241m=\u001b[39m K, lambd \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.2\u001b[39m, \n\u001b[1;32m      8\u001b[0m              alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.05\u001b[39m, beta \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.05\u001b[39m, iterations \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m, \n\u001b[1;32m      9\u001b[0m              lower_control \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-15\u001b[39m, eta \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5.0\u001b[39m, seed \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Fit the model to the graph\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[43mMNMF_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mG\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Obtain the graph embeddings\u001b[39;00m\n\u001b[1;32m     15\u001b[0m embd \u001b[38;5;241m=\u001b[39m MNMF_model\u001b[38;5;241m.\u001b[39mget_embedding()\n",
      "Input \u001b[0;32mIn [77]\u001b[0m, in \u001b[0;36mMNMF.fit\u001b[0;34m(self, graph)\u001b[0m\n\u001b[1;32m    190\u001b[0m graph \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_graph(graph)\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph \u001b[38;5;241m=\u001b[39m graph\n\u001b[0;32m--> 192\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_matrices\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterations):\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_M()\n",
      "Input \u001b[0;32mIn [77]\u001b[0m, in \u001b[0;36mMNMF._setup_matrices\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_C \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39muniform(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclusters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdimensions))\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_B1 \u001b[38;5;241m=\u001b[39m nx\u001b[38;5;241m.\u001b[39madjacency_matrix(\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph, nodelist\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph\u001b[38;5;241m.\u001b[39mnodes())\n\u001b[1;32m     90\u001b[0m )\n\u001b[0;32m---> 91\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_B2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_modularity_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_X \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_U)\n\u001b[1;32m     93\u001b[0m overlaps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_B1\u001b[38;5;241m.\u001b[39mdot(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_B1)\n",
      "Input \u001b[0;32mIn [77]\u001b[0m, in \u001b[0;36mMNMF._modularity_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     70\u001b[0m scores \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28mfloat\u001b[39m(degs[e[\u001b[38;5;241m0\u001b[39m]] \u001b[38;5;241m*\u001b[39m degs[e[\u001b[38;5;241m1\u001b[39m]]) \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m e_count) \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph\u001b[38;5;241m.\u001b[39medges()\n\u001b[1;32m     72\u001b[0m ]\n\u001b[1;32m     73\u001b[0m scores \u001b[38;5;241m=\u001b[39m scores \u001b[38;5;241m+\u001b[39m [\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28mfloat\u001b[39m(degs[e[\u001b[38;5;241m1\u001b[39m]] \u001b[38;5;241m*\u001b[39m degs[e[\u001b[38;5;241m0\u001b[39m]]) \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m e_count) \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph\u001b[38;5;241m.\u001b[39medges()\n\u001b[1;32m     75\u001b[0m ]\n\u001b[0;32m---> 76\u001b[0m mod_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mcoo_matrix\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43mscores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mindices_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices_2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodularity_mat_shape\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m mod_matrix\n",
      "File \u001b[0;32m/N/soft/sles15/deeplearning/Python-3.10.5/lib/python3.10/site-packages/scipy/sparse/_coo.py:196\u001b[0m, in \u001b[0;36mcoo_matrix.__init__\u001b[0;34m(self, arg1, shape, dtype, copy)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 196\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/N/soft/sles15/deeplearning/Python-3.10.5/lib/python3.10/site-packages/scipy/sparse/_coo.py:283\u001b[0m, in \u001b[0;36mcoo_matrix._check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnnz \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrow\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n\u001b[0;32m--> 283\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrow index exceeds matrix dimensions\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcol\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m    285\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcolumn index exceeds matrix dimensions\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: row index exceeds matrix dimensions"
     ]
    }
   ],
   "source": [
    "D=20\n",
    "\n",
    "K = max(intrinsic_membership)+1\n",
    "\n",
    "\n",
    "# Create an instance of the MNMF model\n",
    "MNMF_model = MNMF(dimensions = D, clusters = K, lambd = 0.2, \n",
    "             alpha = 0.05, beta = 0.05, iterations = 100, \n",
    "             lower_control = 1e-15, eta = 5.0, seed = 42)\n",
    "\n",
    "# Fit the model to the graph\n",
    "MNMF_model.fit(G)\n",
    "\n",
    "# Obtain the graph embeddings\n",
    "embd = MNMF_model.get_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c5fa3259",
   "metadata": {},
   "outputs": [],
   "source": [
    "H = nx.relabel.convert_node_labels_to_integers(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7eee78bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NodeView((0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975, 976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998))"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H.nodes()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
